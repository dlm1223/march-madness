---
title: 'Sensivity Analysis, Backtestin, and March Madness'
date: "March 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I made an [app](https://bracketmath.shinyapps.io/ncaa/) to optimize March Madness brackets. In my [last post](https://www.brianlefevre.com/wp-content/uploads/2018/03/march_madness_backtest.html), I examinined its success over the past 8 seasons. The algorithm works by creating a pool of brackets based on that years' ownership percentages, then simulating the tournament, then taking the brackets that finished highly relative to the other brackets over the simulations. With the backtest I could see how the algorithm actually performed, but a problem with bakctesting is that it's easy to overfit. Everything could look amazing with a certain set of parameters, but then you make a slight adjustment and the whole system falls apart. A useful way to lessen overfitting is by doing sensitivity analysis and seeing the performance when making slight adjustments to parameters. In this post I do sensitiy analysis by testing a different projection model.<br />

##Creating New Projections

First I create new projections. My projections are a function of variables such as KenPom-Rank, Sonny Moore-Rank, Vegas Spread, TeamRankings-Rank, Game Distance, and Tournament Seed. I tried to adjust the second model so that it would have variations from the first.

```{r , echo=F, eval=T , include=F}
library(plyr);library(dplyr);library(ggplot2);library(ggrepel)

readFile<-function(year, version=""){
  data<-read.csv(paste(c("~/Kaggle/NCAA/march-madness/",year ,"/Kaggle Submission", version,".csv"), sep="", collapse=""))
  data$Year<-year
  if(version==" v2"){
    colnames(data)<-gsub("pred", "pred2", colnames(data))
  }
  data
}
submissions<-ldply(lapply(2010:2017, readFile), data.frame)
submissions2<-ldply(lapply(2010:2017, function(x) readFile(x, version=" v2")), data.frame)
compare<-merge(submissions, submissions2, by=c("id", "Year"))
```

```{r figA, echo=T, eval=T , include=T,fig.height = 3, fig.width = 5, fig.align = "center"}
plot(compare$pred~compare$pred2, main="Model 1 vs Model 2 Predictions, 2010-2017")
cor(compare$pred, compare$pred2)
```
Looking at the plots and correlations you can see they have some variation. Also, looking at the models' projected tournament winners below, they had different projected winners in 4/8 years, so there is decent variation.

```{r table, echo=F, eval=T , include=T,fig.height = 3, fig.width = 5, fig.align = "center"}
load("compare.Rda")
submissions

```

##Rerunning the Backtest

Now that I see the projections are relatively different, I can rerun the backtest from the last post.

```{r  fig, echo=F, eval=T , include=T, fig.height = 3, fig.width = 5, fig.align = "center"}
load("backtest results v2.Rda")

makePlot<-function(means, title="ROI by Optimization Percentile and numBrackets"){
  a<-ggplot(data=means, aes(fill=numBrackets, y=ROI, x=percentile)) +  
    geom_bar(position="dodge", stat="identity"    ,
             colour="black", # Use black outlines,
             size=.3) +      # Thinner lines
    geom_errorbar(aes(ymin = ROI - seROI, ymax = ROI + seROI),                
                  size=.3,    # Thinner lines
                  width=.2,
                  position=position_dodge(.9)) + 
    ylab("ROI (%)") +
    ggtitle(title)
  print(a)
}
groupedLine<-function(test, title="Cumulative Profit"){
  test[, c("percentile", "numBrackets")]<-sapply(test[, c("percentile", "numBrackets")], as.factor)
  
  a<-test%>% mutate(label = if_else(year == max(year), paste0(winningYears, "/8 Winning"), ifelse(year==min(year), numBrackets, NA_character_))) %>%
    ggplot( aes(x=year, y=CumProfit, group = numBrackets, colour = numBrackets)) +
    geom_line()+
    scale_colour_discrete(guide = 'none') +
    geom_label_repel(aes(label = label),
                     nudge_x = 1,
                     na.rm = TRUE)+
    ggtitle(title)
  print(a)
}
backtest$numWinning<-rowSums(apply(backtest[, grepl("result", colnames(backtest))],2, function(x) x>=backtest$percentile  ), na.rm=T)
backtest$Prize<-(1/(1-backtest$percentile))*backtest$numWinning
backtest$Prize_same<-(1/(1-backtest$percentile))*(backtest$numWinning>=1)
backtest$Entry<-1*backtest$numBrackets


means<-ddply(backtest, .(percentile, numBrackets),
             summarize,ROI=100* sum(Prize_same-Entry)/sum(Entry), seROI=100*sd((Prize_same-Entry)/Entry)/length(year))
means[, c("percentile", "numBrackets")]<-sapply(means[, c("percentile", "numBrackets")], as.factor)
makePlot(means)

backtest<-ddply(backtest, .(numBrackets, percentile),mutate,
                winningYears=sum(Prize_same>0),
                CumProfit=cumsum(Prize_same)-cumsum(Entry))
test<-backtest[backtest$percentile==.9,]
groupedLine(test, title="Cumulative Profit, 90th-percentile parameter")

```

Like the last backtest, the 90th percentile parameter does well, however here the ROI and profit have decreased. These plots were calculated using different pool scoring i.e. 90th-percentile brackets get $10 if >=90th percentile, 99th-percentile brackets get $100 if >=99th percentile. Next I try applying the same scoring prize pool to each set of brackets so that I can compare each bracket set using the same prize pool. First, I apply 20-man pool scoring to everything:

```{r fig2, echo=F, eval=T , include=T,fig.height = 3, fig.width = 5, fig.align = "center"}
percentile<-.95

backtest$numWinning<-rowSums(apply(backtest[, grepl("result", colnames(backtest))],2, function(x) x>=percentile  ), na.rm=T)
backtest$numSecond<-rowSums(apply(backtest[, grepl("result", colnames(backtest))],2, function(x) x<percentile & x>=percentile-(1-percentile) ), na.rm=T)
backtest$Prize<-(1/(1-percentile))*backtest$numWinning
backtest$Prize_same<-(1/(1-percentile))*(backtest$numWinning>=1)


means<-ddply(backtest, .(percentile, numBrackets), summarize,ROI=100*sum(Prize_same-Entry)/sum(Entry), seROI=100*sd((Prize_same-Entry)/Entry)/length(year))
means[, c("percentile", "numBrackets")]<-sapply(means[, c("percentile", "numBrackets")], as.factor)
makePlot(means, title="ROI, Medium Pool Scoring")

backtest<-ddply(backtest, .(numBrackets, percentile),mutate,
                winningYears=sum(Prize_same>0),
                CumProfit=cumsum(Prize_same)-cumsum(Entry))
test<-backtest[backtest$percentile==.9,]
groupedLine(test, title="Cumulative Profit, 90th-percentile parameter")

test<-backtest[backtest$percentile==.95,]
groupedLine(test, title="Cumulative Profit, 95th-percentile parameter")


```

Then, I apply 100-man pool scoring to everything:

```{r fig4, echo=F, eval=T , include=T,fig.height = 3, fig.width = 5, fig.align = "center"}
percentile<-.99

backtest$numWinning<-rowSums(apply(backtest[, grepl("result", colnames(backtest))],2, function(x) x>=percentile  ), na.rm=T)
backtest$numSecond<-rowSums(apply(backtest[, grepl("result", colnames(backtest))],2, function(x) x<percentile & x>=percentile-(1-percentile) ), na.rm=T)
backtest$Prize<-(1/(1-percentile))*backtest$numWinning
backtest$Prize_same<-(1/(1-percentile))*(backtest$numWinning>=1)


means<-ddply(backtest, .(percentile, numBrackets), summarize,ROI=100*sum(Prize_same-Entry)/sum(Entry), seROI=100*sd((Prize_same-Entry)/Entry)/length(year))
means[, c("percentile", "numBrackets")]<-sapply(means[, c("percentile", "numBrackets")], as.factor)
makePlot(means, title="ROI, Large Pool Scoring")

backtest<-ddply(backtest, .(numBrackets, percentile),mutate,
                winningYears=sum(Prize_same>0),
                CumProfit=cumsum(Prize_same)-cumsum(Entry))
test<-backtest[backtest$percentile==.9,]
groupedLine(test, title="Cumulative Profit, 90th-percentile parameter")

test<-backtest[backtest$percentile==.95,]
groupedLine(test, title="Cumulative Profit, 95th-percentile parameter")


```



##Conclusion
The first conclusion I made in the last post was that the 90th percentile parameter gives the best brackets because it has more of a sample size to choose from than the other parameters. Based on the sensitivity analysis, the first conclusion is not clear. The 90th percentile performs well but so does the 95th percentile. The second conclusion I made was that small pools are better because they are less top heavy and will have you winning in more years.  The sensitivity analysis supports this conclusion. Once you get to pools of over 50 people, you really are just playing a lottery, so if you stay within 20-person pools, you will give yourself a really good change. All of this was for 1-2-4-8-16-32 scoring. In a future post I will look at different scoring systems and different payout alternatives to winner-take-all.





